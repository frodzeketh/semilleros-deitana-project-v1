const { OpenAI } = require('openai');
const { Pinecone } = require('@pinecone-database/pinecone');
const { addMessage, getHistory } = require('../../utils/ramMemory');
const { query } = require('../../db-bridge');
require('dotenv').config();

const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY
});

// Configurar Pinecone
const pinecone = new Pinecone({
    apiKey: 'pcsk_ctXEB_EytPZdg6HJhk2HPbfvEfknyuM671AZUmwz82YSMVgjYfGfR3QfsLMXC8BcRjUvY'
});

const index = pinecone.index('deitana-knowledge');

// Funci√≥n para buscar informaci√≥n relevante en Pinecone
async function searchRelevantInfo(query) {
    try {
        // Crear embedding del query
        const embeddingResponse = await openai.embeddings.create({
            model: 'text-embedding-3-small',
            input: query,
            dimensions: 512
        });
        
        const queryEmbedding = embeddingResponse.data[0].embedding;
        
        // Buscar en Pinecone con m√°s resultados
        const searchResponse = await index.query({
            vector: queryEmbedding,
            topK: 10,  // Aumentar de 3 a 10 para m√°s cobertura
            includeMetadata: true
        });
        
        // Extraer informaci√≥n relevante
        console.log('üîç [RAG] Resultados de b√∫squeda:', searchResponse.matches.length);
        const relevantInfo = searchResponse.matches
            .map(match => {
                console.log('üìÑ [RAG] Match encontrado:', match.metadata);
                return match.metadata?.text || match.metadata?.content || '';
            })
            .filter(text => text.length > 0)
            .join('\n\n');
        
        console.log('üìä [RAG] Informaci√≥n relevante encontrada:', relevantInfo.length, 'caracteres');
            
        return relevantInfo;
    } catch (error) {
        console.error('Error buscando en Pinecone:', error);
        return '';
    }
}

async function processQueryStream({ message, conversationId, response }) {
    try {
        console.log('üöÄ [OPENAI] Procesando con memoria RAM y function calling');
        
        // 1. MEMORIA RAM SIMPLE
        const conversationIdFinal = conversationId || `temp_${Date.now()}`;
        const history = getHistory(conversationIdFinal);
        
        // 2. AGREGAR MENSAJE DEL USUARIO
        addMessage(conversationIdFinal, 'user', message);
        
        // 3. BUSCAR INFORMACI√ìN RELEVANTE EN RAG
        const relevantInfo = await searchRelevantInfo(message);
        
        // 4. CREAR PROMPT CON CONTEXTO DE LA EMPRESA
        const systemPrompt = `Eres un asistente especializado de Semilleros Deitana, S.L. 

INFORMACI√ìN ESPEC√çFICA DE LA EMPRESA:
${relevantInfo}

INSTRUCCIONES CR√çTICAS:
- SIEMPRE usa la informaci√≥n espec√≠fica de Deitana que se te proporciona arriba
- Si encuentras informaci√≥n relevante en el contexto, √∫sala como base de tu respuesta
- Responde de manera completa y detallada
- Si la informaci√≥n espec√≠fica no est√° disponible, menciona que no tienes esa informaci√≥n espec√≠fica
- Mant√©n un tono profesional y cercano
- Responde siempre en espa√±ol

CAPACIDADES ESPECIALES - EJECUCI√ìN DE SQL:
- Tienes acceso a la funci√≥n execute_sql para consultar la base de datos MySQL
- USA execute_sql cuando el usuario pregunte por:
  * Cantidades: "cu√°ntos clientes", "cu√°ntos vendedores", "cu√°ntos art√≠culos"
  * Listados: "listar clientes", "mostrar vendedores", "art√≠culos con stock bajo"
  * Datos espec√≠ficos: "clientes de Madrid", "vendedores activos", "stock de tomates"
- NO uses execute_sql para preguntas conceptuales como "qu√© es un cipr√©s" o "c√≥mo funciona el injerto"
- IMPORTANTE: Usa nombres de tablas sin comillas o con comillas simples, NO comillas dobles
- Despu√©s de ejecutar SQL, explica los resultados de manera clara y √∫til

IMPORTANTE: La informaci√≥n de arriba es espec√≠fica de Semilleros Deitana. √ösala para dar respuestas precisas sobre la empresa.`;

        // 5. PREPARAR MENSAJES CON HISTORIAL Y FUNCIONES
        const messages = [
            { role: 'system', content: systemPrompt },
            ...history, // Historial completo desde RAM
            { role: 'user', content: message }
        ];

        console.log(`üí¨ [RAM] Enviando ${messages.length} mensajes a GPT-4o con function calling`);

        // UNA SOLA LLAMADA - STREAMING CON FUNCTION CALLING
        const stream = await openai.chat.completions.create({
            model: 'gpt-4o',
            messages: messages,
            tools: [
                {
                    type: 'function',
                    function: {
                        name: 'execute_sql',
                        description: 'Ejecuta una consulta SQL en la base de datos de Semilleros Deitana',
                        parameters: {
                            type: 'object',
                            properties: {
                                query: {
                                    type: 'string',
                                    description: 'La consulta SQL a ejecutar'
                                }
                            },
                            required: ['query']
                        }
                    }
                }
            ],
            tool_choice: 'auto',
            stream: true,
            max_tokens: 2000
        });

        // 6. STREAMING CON DETECCI√ìN DE FUNCTION CALLS
        let assistantResponse = '';
        let functionName = null;
        let functionArguments = '';
        let toolCallId = null;
        
        for await (const chunk of stream) {
            const delta = chunk.choices[0]?.delta;
            
            // Manejar contenido de texto
            if (delta?.content) {
                assistantResponse += delta.content;
                const jsonChunk = JSON.stringify({ type: 'chunk', content: delta.content }) + '\n';
                console.log('üì§ [STREAM] Enviando chunk:', delta.content.substring(0, 50) + '...');
                response.write(jsonChunk);
            }
            
            // Manejar llamadas a funciones
            if (delta?.tool_calls) {
                for (const toolCall of delta.tool_calls) {
                    if (toolCall.function) {
                        if (toolCall.function.name) {
                            functionName = toolCall.function.name;
                            console.log('üîß [FUNCTION] Funci√≥n detectada:', functionName);
                        }
                        if (toolCall.function.arguments) {
                            functionArguments += toolCall.function.arguments;
                        }
                        if (toolCall.id) {
                            toolCallId = toolCall.id;
                        }
                    }
                }
            }
        }

        // 7. EJECUTAR SQL SI SE DETECT√ì
        console.log('üîç [DEBUG] functionName:', functionName);
        console.log('üîç [DEBUG] functionArguments:', functionArguments);
        console.log('üîç [DEBUG] toolCallId:', toolCallId);
        
        if (functionName === 'execute_sql' && functionArguments) {
            try {
                console.log('üîç [DEBUG] Argumentos completos:', functionArguments);
                const args = JSON.parse(functionArguments);
                let sqlQuery = args.query;
                
                // Arreglar comillas dobles por simples para MySQL
                sqlQuery = sqlQuery.replace(/"/g, '`');
                
                console.log('‚ö° [SQL] Ejecutando SQL:', sqlQuery);
                
                // Ejecutar SQL
                const sqlResults = await query(sqlQuery);
                console.log('üìä [SQL] Resultados obtenidos:', sqlResults.length, 'filas');
                
                // Continuar la conversaci√≥n con los resultados SQL para que el modelo responda inteligentemente
                const continuationMessages = [
                    { role: 'system', content: systemPrompt }, // Incluir system prompt en la segunda llamada
                    ...messages,
                    { role: 'assistant', content: assistantResponse, tool_calls: [{ type: 'function', function: { name: functionName, arguments: functionArguments }, id: toolCallId }] },
                    { role: 'tool', content: JSON.stringify(sqlResults), tool_call_id: toolCallId }
                ];
                
                console.log('üîÑ [CONTINUATION] Enviando resultados SQL al modelo para respuesta inteligente');
                
                // Segunda llamada para que el modelo responda con los datos
                const continuationStream = await openai.chat.completions.create({
                    model: 'gpt-4o',
                    messages: continuationMessages,
                    stream: true,
                    max_tokens: 1000
                });
                
                // Stream de la respuesta inteligente del modelo
                for await (const chunk of continuationStream) {
                    const content = chunk.choices[0]?.delta?.content || '';
                    if (content) {
                        assistantResponse += content;
                        const jsonChunk = JSON.stringify({ type: 'chunk', content }) + '\n';
                        console.log('üì§ [STREAM] Enviando chunk inteligente:', content.substring(0, 50) + '...');
                        response.write(jsonChunk);
                    }
                }
                
                console.log('‚úÖ [SQL] Function calling completado - modelo respondi√≥ inteligentemente');
                
                // Guardar respuesta en memoria
                addMessage(conversationIdFinal, 'assistant', assistantResponse);
                console.log('üíæ [RAM] Respuesta SQL formateada guardada en memoria');
                
                // Enviar mensaje de finalizaci√≥n
                const endChunk = JSON.stringify({ type: 'end', conversationId: conversationIdFinal }) + '\n';
                response.write(endChunk);
                console.log('üîö [END] Enviando mensaje de finalizaci√≥n');
                
                response.end();
                return;
                
                } catch (error) {
                console.error('‚ùå [SQL] Error en function calling:', error);
                const errorChunk = JSON.stringify({ type: 'chunk', content: `Error al ejecutar la consulta: ${error.message}` }) + '\n';
                response.write(errorChunk);
                response.end();
                return;
            }
        }

        // 8. GUARDAR RESPUESTA FINAL EN RAM
        if (assistantResponse.trim()) {
            addMessage(conversationIdFinal, 'assistant', assistantResponse);
            console.log('üíæ [RAM] Respuesta final guardada en memoria');
            console.log('üì§ [FINAL] Respuesta completa:', assistantResponse.substring(0, 200) + '...');
        }

        // Enviar mensaje de finalizaci√≥n
        const endChunk = JSON.stringify({ type: 'end', conversationId: conversationIdFinal }) + '\n';
        response.write(endChunk);
        console.log('üîö [END] Enviando mensaje de finalizaci√≥n');
        
        response.end();
            
                } catch (error) {
        console.error('Error:', error);
        if (!response.headersSent) {
            response.status(500).json({ error: 'Error al procesar la consulta' });
        }
    }
}

module.exports = {
    processQueryStream
};